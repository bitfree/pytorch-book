{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 자연어 처리의 일반적인 프로세스\n",
    "\n",
    "> 3.4.2 장에 해당하는 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# Code 4.2.1\n",
    "\n",
    "eng_sentence = \"the quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "## 방법 1. SpaCy\n",
    "# pip install spacy\n",
    "# python -m spacy download en\n",
    "import spacy\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "print(tokenize_en(eng_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# Code 4.2.1\n",
    "\n",
    "## 방법 2. nltk\n",
    "# conda install nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(eng_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# Code 4.2.1\n",
    "\n",
    "## 방법 3. split\n",
    "\n",
    "print(eng_sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '날씨', '좋', '다']\n",
      "[('오늘', 'MAG'), ('날씨', 'NNG'), ('좋', 'VA'), ('다', 'EC')]\n",
      "[('오늘', '일반 부사'), ('날씨', '일반 명사'), ('좋', '형용사'), ('다', '연결 어미')]\n"
     ]
    }
   ],
   "source": [
    "# Code 4.2.3\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "kor_sentence = \"오늘 날씨 좋다\"\n",
    "tokenizer = Mecab()\n",
    "# morphs 함수로 형태소 분석을 할 수 있다.\n",
    "print(tokenizer.morphs(kor_sentence))\n",
    "\n",
    "# pos 함수로 품사 태깅을 한다.\n",
    "tokenized_sent = tokenizer.pos(kor_sentence)\n",
    "print(tokenized_sent)\n",
    "# 영어로 된 품사정보를 한글로 전환\n",
    "print([(k, tokenizer.tagset.get(v)) for k, v in tokenized_sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어장\n",
    "\n",
    "### 네이버 댓글 데이터의 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['안개', '자욱', '한', '밤하늘', '에', '떠', '있', '는', '초승달', '같', '은', '영화', '.'], 1)\n"
     ]
    }
   ],
   "source": [
    "# Code 4.2.4\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "# 필자의 데이터 저장위치: ./data/nsmc/\n",
    "# 데이터 불러오기: 첫 20 문장만 선택\n",
    "with open(\"./data/nsmc/ratings.txt\") as file:\n",
    "    # 행단위로 데이터를 분리한다. 첫 열은 댓글에 해당하는 id 가 있음으로 제외한다.\n",
    "    raw_data = file.read().splitlines()[1:]\n",
    "    # 텍스트 데이터와 라벨을 분리한다.\n",
    "    data = [line.split(\"\\t\")[1:] for line in raw_data]\n",
    "    # 토큰화를 진행한다.\n",
    "    data = [(tokenizer.morphs(sent), int(label)) for (sent, label) in data]\n",
    "    \n",
    "# 10개의 긍정 댓글과 10 개의 부정적인 댓글을 샘플링 한다.\n",
    "sample_data = data[4:14] + data[-10:]\n",
    "print(sample_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 4.2.5\n",
    "\n",
    "def build_vocab(data):\n",
    "    # flatten 함수는 list 안의 list 를 하나의 리스트로 풀어준다\n",
    "    flatten = lambda d: [token for sent in d for token in sent]\n",
    "    vocab = {}\n",
    "    # unkown 과 pad 토큰을 설정한다\n",
    "    vocab['<unk>'] = 0\n",
    "    vocab['<pad>'] = 1\n",
    "    # 단어장을 생성한다.\n",
    "    for token in flatten(list(zip(*data))[0]):\n",
    "        if vocab.get(token) is None:\n",
    "            vocab.setdefault(token, len(vocab))\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수치화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 4.2.6\n",
    "\n",
    "def numericalize(sent, vocab):\n",
    "    temp = []\n",
    "    for token in sent:\n",
    "        if vocab.get(token) is None:\n",
    "            temp.append(vocab['<unk>'])\n",
    "        else:\n",
    "            temp.append(vocab.get(token))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 1)\n"
     ]
    }
   ],
   "source": [
    "# Code 4.2.6\n",
    "\n",
    "numerical_data = [(numericalize(sent, vocab), label) for sent, label in sample_data]\n",
    "print(numerical_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 4.2.7\n",
    "\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "num_docs = len(numerical_data)\n",
    "num_vocab = len(vocab)\n",
    "term_matrix = torch.zeros(num_docs, num_vocab)\n",
    "\n",
    "for i, (sent, label) in enumerate(numerical_data):\n",
    "    tokens, cnts = zip(*list(Counter(sent).items()))\n",
    "    term_matrix[i, torch.LongTensor(tokens)] = torch.FloatTensor(cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 157])\n",
      "tensor(2917)\n"
     ]
    }
   ],
   "source": [
    "# Code 4.2.7\n",
    "\n",
    "print(term_matrix.size())\n",
    "print(term_matrix.eq(0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heaps' Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fig 4.2.1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "global_unique = []\n",
    "global_unique_cnt = []\n",
    "for sent in data[:10000]:\n",
    "    unique = set(sent[0])\n",
    "    global_unique += list(unique)\n",
    "    global_unique = list(set(global_unique))\n",
    "    global_unique_cnt.append(len(global_unique))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(list(range(10000)), global_unique_cnt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Length of Sentences: \n",
      "[13, 17, 6, 17, 1, 8, 38, 2, 6, 20, 19, 18, 3, 9, 11, 6, 5, 15, 15, 8]\n",
      "After Length of Sentences: \n",
      "[38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38]\n",
      "([2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 1)\n"
     ]
    }
   ],
   "source": [
    "# Code 4.2.8\n",
    "\n",
    "print(\"Before Length of Sentences: \")\n",
    "print([len(sent) for sent, _ in numerical_data])\n",
    "max_len = max([len(sent) for (sent, _) in numerical_data])\n",
    "# 패딩과정\n",
    "for sent, _ in numerical_data:\n",
    "    if len(sent) < max_len:\n",
    "        sent += [vocab['<pad>']] * (max_len - len(sent))\n",
    "print(\"After Length of Sentences: \")\n",
    "print([len(sent) for sent, _ in numerical_data])\n",
    "print(numerical_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(torchenv) $ pip install torchtext\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 4.2.9\n",
    "\n",
    "from torchtext.data import Field\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# 토큰화 함수로 MeCab 사용\n",
    "tokenizer = Mecab()\n",
    "\n",
    "\n",
    "# 필드 정의\n",
    "TEXT = Field(sequential=True,\n",
    "             use_vocab=True,\n",
    "             tokenize=tokenizer.morphs,  \n",
    "             lower=True, \n",
    "             batch_first=True)  \n",
    "LABEL = Field(sequential=False,  \n",
    "              use_vocab=False,   \n",
    "              preprocessing = lambda x: int(x),  \n",
    "              batch_first=True, \n",
    "              is_target=True)\n",
    "\n",
    "# 각 댓글에 해당하는 id, 사용하지 않지만 기본필드로 정의 해준다.\n",
    "ID = Field(sequential=False,  \n",
    "           use_vocab=False,   \n",
    "           is_target=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 4.2.10\n",
    "\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "dataset = TabularDataset(path='./data/nsmc/ratings.txt', \n",
    "                         format='tsv', \n",
    "                         fields=[('id', ID), ('text', TEXT), ('label', LABEL)],\n",
    "                         skip_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 60825\n",
      "Token for \"<unk>\": 0\n",
      "Token for \"<pad>\": 1\n"
     ]
    }
   ],
   "source": [
    "# Code 4.2.11\n",
    "\n",
    "TEXT.build_vocab(dataset)\n",
    "print('Total vocabulary: {}'.format(len(TEXT.vocab)))\n",
    "print('Token for \"<unk>\": {}'.format(TEXT.vocab.stoi['<unk>']))\n",
    "print('Token for \"<pad>\": {}'.format(TEXT.vocab.stoi['<pad>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로더 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 12]) torch.Size([3])\n",
      "tensor([[16861, 16855,    69, 16861, 16855,     2,    17,     1,     1,     1,\n",
      "             1,     1],\n",
      "        [   28,    10,  5451,     4,  1952,    22,    53,    41,    21,    21,\n",
      "         14742, 12032],\n",
      "        [ 6520,  2376,   365,    10,  1301,  3393,    13,  2394,     1,     1,\n",
      "             1,     1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Code 4.2.12\n",
    "\n",
    "from torchtext.data import Iterator\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data_loader = Iterator(dataset=dataset, \n",
    "                       batch_size=3,\n",
    "                       device=device)\n",
    "\n",
    "for batch in data_loader:\n",
    "    break\n",
    "\n",
    "# 필드에서 정의한 이름으로 호출 할 수 있다.\n",
    "print(batch.text.size(), batch.label.size())\n",
    "print(batch.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(torchenv)",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
